const OpenAI = require('openai');
const fs = require('fs');
const path = require('path');

// Charger les variables d'environnement
require('dotenv').config({ path: path.join(__dirname, '.env.local') });

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

async function testOpenAIUsage() {
  console.log('ðŸ§ª Test des donnÃ©es d\'usage OpenAI...');
  console.log('ModÃ¨le testÃ©: gpt-5-2025-08-07');
  
  try {
    // Test 1: Appel simple sans streaming
    console.log('\nðŸ“Š Test 1: Appel simple sans streaming');
    const simpleCompletion = await openai.chat.completions.create({
      model: 'gpt-5-2025-08-07',
      messages: [{ role: 'user', content: 'Dis bonjour en une phrase.' }],
      max_completion_tokens: 50
    });
    
    console.log('Usage data (simple):', simpleCompletion.usage);
    console.log('Response:', simpleCompletion.choices[0].message.content);
    
    // Test 2: Appel avec streaming et include_usage
    console.log('\nðŸ“Š Test 2: Appel avec streaming et include_usage');
    const streamCompletion = await openai.chat.completions.create({
      model: 'gpt-5-2025-08-07',
      messages: [{ role: 'user', content: 'Dis bonjour en une phrase.' }],
      max_completion_tokens: 50,
      stream: true,
      stream_options: { include_usage: true }
    });
    
    let streamUsage = null;
    let content = '';
    let chunkCount = 0;
    
    for await (const chunk of streamCompletion) {
      chunkCount++;
      
      if (chunk.choices[0]?.delta?.content) {
        content += chunk.choices[0].delta.content;
      }
      
      if (chunk.usage) {
        streamUsage = chunk.usage;
        console.log(`Usage data trouvÃ© dans le chunk ${chunkCount}:`, chunk.usage);
      }
      
      if (chunk.choices[0]?.finish_reason) {
        console.log('Stream terminÃ©, finish_reason:', chunk.choices[0].finish_reason);
        break;
      }
    }
    
    console.log('Usage data final (stream):', streamUsage);
    console.log('Response (stream):', content);
    console.log('Nombre total de chunks:', chunkCount);
    
    // Test 3: VÃ©rifier avec un autre modÃ¨le pour comparaison
    console.log('\nðŸ“Š Test 3: Comparaison avec gpt-4o-mini');
    const gpt4oCompletion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: 'Dis bonjour en une phrase.' }],
      max_tokens: 50,
      stream: true,
      stream_options: { include_usage: true }
    });
    
    let gpt4oUsage = null;
    let gpt4oContent = '';
    let gpt4oChunkCount = 0;
    
    for await (const chunk of gpt4oCompletion) {
      gpt4oChunkCount++;
      
      if (chunk.choices[0]?.delta?.content) {
        gpt4oContent += chunk.choices[0].delta.content;
      }
      
      if (chunk.usage) {
        gpt4oUsage = chunk.usage;
        console.log(`Usage data GPT-4o-mini dans le chunk ${gpt4oChunkCount}:`, chunk.usage);
      }
      
      if (chunk.choices[0]?.finish_reason) {
        break;
      }
    }
    
    console.log('Usage data final (gpt-4o-mini):', gpt4oUsage);
    console.log('Response (gpt-4o-mini):', gpt4oContent);
    
  } catch (error) {
    console.error('âŒ Erreur lors du test:', error.message);
    if (error.response) {
      console.error('Response status:', error.response.status);
      console.error('Response data:', error.response.data);
    }
  }
}

// ExÃ©cuter le test
testOpenAIUsage().then(() => {
  console.log('\nâœ… Test terminÃ©');
}).catch(console.error);